%Modify the relative path accordingly
\input{common.tex}

\fancyhead[LO,LE]{HU, Pili}
\fancyhead[RO,RE]{Spectral Clustering Survey}

%This usually doesn't need modification 
\author{HU, Pili\thanks{hupili [at] ie [dot] cuhk [dot] edu [dot] hk}}

%Modify them accordingly===
\title{Spectral Clustering Survey}
\date{May 14, 2012\thanks{Last compile:\today}}

\begin{document}

\maketitle
%>============================================
\begin{abstract}
	Abstract. 
\end{abstract}
%<=======Abstract ENd=========================

%>============================================
\pagebreak
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
%<=======TOC ENd==============================



\section{Introduction}
\label{sec:introduction}

Spectral Clustering(SC) was used in several disciplines long ago. 
For example, computer vision\cite{shi2000normalized}, load balancing
\cite{hendrickson1993multidimensional}, electronics design
\cite{hadley1992efficient}, etc.  Spectral Embeding(SE) was 
also widely discussed in the community\cite{brand2003unifying}. 
Outside spectral community, the machine learning community also 
developed many linear or non-linear Dimensionality Reduction(DR) methods, 
like Principal Component Anslysis (PCA), Kernel PCA (KPCA)\cite{scholkopf1998kpca}, 
Locally Linear Embedding (LLE)\cite{roweis2000lle}, etc. 
Other technique like Multi-Dimensional Scaling(MDS) was successfully 
used in computational psychology for a very long time\cite{borg2005modern}, 
which can be viewed as both "embedding" or "dimensionality reduction".


According to our survey, although those methods target at different problem
and are derived from different assumptions, they do share a lot in common. 
The most significant sign is that, the core procedure involves 
eigenvalue decomposition or singular value decomposition, aka "spectral". 
They all involve an intermidiate step of embedding high-dimensional / 
non-Euclidean / non-metric points into a low-dimensional Euclidean space
(although some do not embed explicitly). In this case, we categorize all 
these algorithms as Spectral Embedding Technique(SET). 

\subsection{A Sample Spectral Clustering Algorithm}

\subsection{Linear Algebraic Properties}

\section{Spectral Clustering Framework}

\subsection{Metric Formulation}

\subsection{Spectral Embeding}

\subsection{Clustering}


\section{Spectral Clustering Justification}
\label{sec:justification}

\subsection{Random Walk}
[von]

\subsection{Normalized Cut}
[shi]

\subsection{Ratio Cut}
[von]

\subsection{Conductance}
[von]

\subsection{Matrix Perturbation}
[andrew ng]

\subsection{Low Rank Approximation}
[matthew brand]

\subsection{Density Estimation View}
[mo chen, 2010]

\subsection{Commute Time}
[jihun ham, kernel]. 
view pseudo inverse of graph Laplacian by commute times on graphs. 

\section{Other Spectral Like Embedding}
\label{sec:nldr}

\subsection{MDS}

\subsection{isomap}

\subsection{Laplacian Eigenmap}

\subsection{Hessian Eigenmap}

\subsection{PCA}

\subsection{LLE}

\subsection{Kernel PCA}

\subsection{Kernel Framework}

\subsection{Graph Framework}


\section{Conclusion}
\label{sec:conclusion}



%>============================================
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

%<=======Acknowledgements ENd=================

%>============================================
\addcontentsline{toc}{section}{References}
\input{../reference/gen_bib.bbl}
%<=======Bibliography ENd=====================

%>============================================
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

%<=======Appendix ENd=========================

\end{document}
