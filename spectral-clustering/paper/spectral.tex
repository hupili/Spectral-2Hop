%Modify the relative path accordingly
\input{common.tex}

\usepackage{algorithm}
\usepackage{algorithmic}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

%notation shortcuts
\newcommand{\nL}{\mathcal{L}}
\newcommand{\nA}{\mathcal{A}}

\usepackage{subfigure}

\fancyhead[LO,LE]{HU, Pili}
\fancyhead[RO,RE]{Spectral Clustering Survey}

%This usually doesn't need modification 
\author{HU, Pili\thanks{hupili [at] ie [dot] cuhk [dot] edu [dot] hk}}

%Modify them accordingly===
\title{Spectral Clustering Survey}
\date{May 14, 2012\thanks{Last compile:\today}}

\begin{document}

\maketitle
%>============================================
\begin{abstract}
	Abstract. Sources can be found in \cite{hu2012-spectral2hop}. 
\end{abstract}
%<=======Abstract ENd=========================

%>============================================
\pagebreak
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
%<=======TOC ENd==============================



\section{Introduction}
\label{sec:introduction}

Spectral Clustering(SC) was used in several disciplines long ago. 
For example, computer vision\cite{shi2000normalized}.
Spectral Embeding(SE) was 
also widely discussed in the community\cite{brand2003unifying}. 
Outside spectral community, the machine learning community also 
developed many linear or non-linear Dimensionality Reduction(DR) methods, 
like Principal Component Anslysis (PCA), Kernel PCA (KPCA)\cite{scholkopf1998kpca}, 
Locally Linear Embedding (LLE)\cite{roweis2000lle}, etc. 
Other technique like Multi-Dimensional Scaling(MDS) was successfully 
used in computational psychology for a very long time\cite{borg2005modern}, 
which can be viewed as both "embedding" or "dimensionality reduction".

%, load balancing
%\cite{hendrickson1993multidimensional}, electronics design
%\cite{hadley1992efficient}, etc.

According to our survey, although those methods target at different problems
and are derived from different assumptions, they do share a lot in common. 
The most significant sign is that, the core procedure involves 
Eigen Value Decomposition(EVD) or Singular Value Decomposition(SVD), aka "spectral". 
They all involve an intermidiate step of embedding high-dimensional / 
non-Euclidean / non-metric points into a low-dimensional Euclidean space
(although some do not embed explicitly). In this case, we categorize all 
these algorithms as Spectral Embedding Technique(SET). 


\subsection{A Sample Spectral Clustering Algorithm}

%%Following is just an example copied from:
%%http://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode
%\begin{algorithm}                      % enter the algorithm environment
%\caption{Calculate $y = x^n$}          % give the algorithm a caption
%\label{alg1}                           % and a label for \ref{} commands later in the document
%\begin{algorithmic}                    % enter the algorithmic environment
%    \REQUIRE $n \geq 0 \vee x \neq 0$
%    \ENSURE $y = x^n$
%\end{algorithmic}
%\end{algorithm}

There are many variations of SC. They all work under certain conditions
and researchers don't have a rule of thumb so far. Before we analyze their
procedure and justification, we present a simple but workable sample 
algorithm(\ralg{\ref{alg:sc_sample}}). 

\begin{algorithm}[htb]
	\caption{Sample Spectral Clustering}
	\label{alg:sc_sample}
	\begin{algorithmic}[1]
		\REQUIRE Data matrix $X = [x_1, x_2, \ldots, x_N]$;  
		Number of Clusters $K$. 
		\ENSURE Clustering $\{C_i\}$: $C_i \in V$ 
			and $\cap_i C_i = \emptyset$
			and $\cup_i C_i = V$. 
		\STATE Form adjacency matrix $A$ within $\epsilon$-ball.
		\STATE Solve $A = U \Lambda \tran{U}$, indexed according 
		the eigenvalue's magnitude. 
		\STATE $Y \leftarrow$ first $K$ columns of U. 
		\STATE Cluster $Y$'s rows by K-means. 
	\end{algorithmic}
\end{algorithm}

In \ralg{\ref{alg:sc_sample}}, the $\epsilon$-ball adjacency graph is 
constructed as follows. First create one vertex for each data point. 
If for two points $i,j$ satisfy $||x_i-x_j|| < \epsilon$, connect 
them with an edge. In this simple demonstration, we consider an unweighted
graph, i.e. all entries of $A$ are 0(disconnected) or 1(connected). 

\begin{figure}
	\centering
	\subfigure[Data Scatter Plot]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_scatter.png}	
		\label{fig:ssc_data}
	}
	\subfigure[Standard K-means]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_kmeans.png}	
		\label{fig:ssc_kmeans}
	}
	\subfigure[Adjacency Graph]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_adj.png}	
		\label{fig:ssc_adj}
	}
	\subfigure[Sample SC Algorithm]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_sc.png}	
		\label{fig:ssc_sc}
	}
	\caption{Demonstration of Sample SC Algorithm}
	\label{fig:ssc_demo}
\end{figure}

\rfig{\ref{fig:ssc_demo}} demonstrates the result of our sample SC algorithm, 
compared with standard K-means algorithm. 
\rfig{\ref{fig:ssc_data}} shows the scatter plot of data. 
It is composed of one radius 1 circle and another radius 2 circle, 
both centered at (1,1). 
\rfig{\ref{fig:ssc_kmeans}} shows the result of standard K-means
working on Euclidean distance. 
\rfig{\ref{fig:ssc_adj}} shows the graph representation, where 
the adjacency graph is formed by taking a $\epsilon$-ball and 
$\epsilon=0.7$ in the example. 
\rfig{\ref{fig:ssc_sc}} shows the output of \ralg{\ref{alg:sc_sample}}. 
It's obvious that standard K-means algorithm can not correctly cluster 
the two circles. This is a known major weakness of K-means(in Euclidean): 
When clusters are not well separated spheres, it has difficulty recovering 
the underlying clusters. Although K-means works for this case 
if we transform the points 
into polar coordinate system(see \cite{hu2012-spectral2hop} for code), 
the solution is not universal. However, in this example, our sample SC 
algorithm can separate the two clusters, probably because the eigenvectors
of adjacency matrix convey adequate information. 

A precaution is that \ralg{\ref{alg:sc_sample}} does not always work even 
in this simple case. Nor have we seen this algorithm from formally published works
(so far), let alone justifications. This algorithm is only to show the flavour 
of spectral clustering and it contains those important steps in other
more sophisticated algorithms. Readers are recommended to learn 
von Luxburg's tutorial\cite{von2007tutorial} before reading the following sections. 
Since that paper is very detailed, we'll present overlapping topics 
concisely. 


\subsection{Spectral Clustering Taxonomy}

When the readers start to survey SC related topics, they will soon find that
there are two streams of study:
\begin{itemize}
	\item \textbf{Embedding Like}. 
		An example is presented in 
		\ralg{\ref{alg:sc_sample}}. One of the core procedure is 
		an embedding of points into lower dimensional Euclidean space. 
		After that, hard cut algorithm like K-means is invoked to 
		get the final result. 		
		This stream has a lot in common with SE and
		DR. In the current article, we focus on this line of research. 
	\item \textbf{Partitioning Like}. 
		One early example is the 
		2-way cut algorithm presented by Shi and Malik in\cite{shi2000normalized}.
		Later Kannan et al. analyzed the framework further\cite{kannan2004clusterings}. 
		The core procedure of this type of algorithm is a 2-way 
		partitioning subroutine. By invoke this subroutine on 
		resultant subgraph repeatedly, we can get final $K$ clusters. 
		When the subroutine can guarantee certain quality, 
		the global resultant clustering quality can be guaranteed
		\cite{kannan2004clusterings}. The attracting aspect of Kannan's framework
		is that, we can plugin any subroutine as long as they have 
		quality gurantee in one cut. For example, the eigen vector of 
		left normalized graph Laplacian can induce good 2-way cut 
		in terms of the Normalized Cut\cite{shi2000normalized}
		(\rsec{\ref{sec:ncut}}). This line of research is more close 
		to Spectral Graph Partitioning(SGP), although those algorithms also get the name 
		of Spectral Clustering. 
\end{itemize}

It's worth to note some work of SGP. In Spielman's\cite{spielman-2009spectral-ln} 
and Lau's\cite{lau-2012-spectral-ln} lecture notes, they both presented a 
graph partitioning algorithm said to result from Lov{\'a}sz\cite{lovasz1990mixing}. 
Chung\cite{chung2007random} made this framework clear: First define a function 
$f(v)$ on all vertices; Then set threshold $T$ to cut vertices into two groups, 
$\{v|f(v) \le T\}$ and $\{v|f(v) > T\}$. The question now becomes to find a 
good heuristic function $f$. Note that the original bi-partition problem is of
complexity $O(2^N)$ and the heuristic based bi-partition problem is of 
$O(N)$(plus the time to get $f$). If we define $f$ as the second eigen vector 
of left normalized graph Laplacian, it is just the algorithm of Shi and
Malik\cite{shi2000normalized}. Besides, there can be multiple $f_i$ and 
the best cut can still be searched in polynomial time
(given $f_i$). For example, in 
\cite{lau-2012-spectral-ln} Lau used random walk probability 
$P_1, P_2, \ldots, P_t$ as $f$. Some recent research of the heuristic function
are Personalized Pagerank\cite{andersen2007detecting}, 
Evolving Sets\cite{andersen2009finding}, etc. 

In the rest of this article, we refer Spectral Clustering to the 
first type, i.e. the embedding like algorithms. 


\subsection{Linear Algebraic Properties}



\section{Spectral Clustering Framework}
\label{sec:framework}

We propose the following framework to absorb currently 
surveyed variations of SC(and other SET):
\begin{enumerate}
	\item \textbf{Metric Formulation}. 
		This step forms a pairwise metric, upon which 
		an adjacency matrix of (weighted) graph can be 
		constructed. 
		There are several kinds of 
		input: high-dimensional data (usual case); pairwise proximity  
		input (like MDS, see \rsec{\ref{sec:mds}}); (weighted) graph
		(like the input of SGP). 
		If the input of SC is already a graph, 
		this step is omitted.
		For proximity measures, especially dissmilarity, it is 
		usually first converted to approximte pairwise inner product in 
		Euclidean space. The pairwise inner product is a positive 
		related quantity with similarity(e.g. Jaccard's coefficient 
		for 0/1 vector\cite{wiki_jaccard}), and thus suits the notion 
		of weights of graph edges. For high-dimensional data, there are 
		more freedom in the metric formulation stage, like 
		similarity graph\cite{von2007tutorial}, geodesic 
		distance\cite{tenenbaum2000isomap}, etc. 
	\item \textbf{Sepctral Embedding}. 
		With the adjacency matrix built in last stage, 
		this stage embeds all vertices into a lower dimensional 
		Euclidean space. For SC community, this embedding 
		makes clustering structure more clear, so that 
		simple algorithms working in Euclidean space can detect 
		the clusters. For DR community, this embedding reveals 
		the shape of manifolds in their parametric space. 
		The two goals are essentially the same. The core procedure 
		is to do EVD and differences 
		lie before and after EVD:
			\begin{itemize}
				\item \textbf{Matrix Type}. Some authors use
					graph Laplacian
					\cite{von2007tutorial,belkin2003laplacian,shi2000normalized} ;
					others use \cite{ng2002spectral,brand2003unifying,kannan2004clusterings}
					adjacency matrix. 
				\item \textbf{Normalization}. Both Laplacian and 
					adjacency matrix can be unnormalized, symmetrically normalized, 
					or left(row) normalized\cite{von2007tutorial}. They corresponds to 
					different interpretation and will be explored later. 
				\item \textbf{Scaling}. 
					As is shown in \ralg{\ref{alg:sc_sample}}, we can directly 
					embed vertices using the corresponding row of $Y$
					(like \cite{ng2002spectral}). Other alternatives 
					are to scale by square root eigenvalue (like \cite{brand2003unifying})
					and scale by eigenvalue (like PCA\cite{bishop2006pattern}). 
				\item \textbf{Projection}. For many algorithms, the $Y$ 
					(after scaling) provides an Euclidean space embedding. 
					There are others which further project the rows of $Y$
					onto a unit sphere, like \cite{ng2002spectral} and
					\cite{brand2003unifying}. 
			\end{itemize}
	\item \textbf{Clustering}. 
		Based on the embedding result, simple algorithms can be invoked to 
		perform a hard cut on the points. Traditional methods from data mining
		community are K-means and hierarchical clustering like single/complete
		linkage\cite{jiawei2001data}. Among those techniques, K-means are the most 
		widely used. A variation of K-means will be proposed later in this article,
		in order to better fit some angle preserving SET. Simpler 
		hard cut techniques are also possible, e.g. looking at the largest entry 
		of the embedding coordinates\cite{kannan2004clusterings}.  
\end{enumerate}

Note that not all of the combinations are justified in published works. 
%		What's more, different normalization with different scaling may yield
%		the same result. 
We organize them in this way to reveal possibilities from a practitioners
perspective. If some combinations yield good results in practice, we can 
seek for justifications using tools from spectral graph theory or machine learning. 


\subsection{Metric Formulation}

We rate great importance of metric formulation step, 
although it is not the main body of SET. 
There are always many ways of metric formulation
given a practical problem. With poorly constructed 
metric matrix, even the best embedding technique helps little.  

%This is a test of math fonts!
%Yeah! I found the notation for normalized 
%adjacency and Laplacian is just the 
%\mathcal style!
%$   \mathit{L}
%    \mathrm{L}
%    \mathbf{L}
%    \mathsf{L}
%    \mathtt{L}
%    \mathcal{L}$

\subsubsection{High Dimensional Data}

For high dimensional data, the following ways can be 
applied to obtain a graph representation:
\begin{itemize}
	\item $\epsilon$-ball\cite{von2007tutorial}. If $||x_i-x_j|| < \epsilon$, we connect 
		vertices $i,j$ with an edge. 
	\item k-Nearest-Neighbour(kNN)\cite{von2007tutorial}. For each vertex, we connect 
		it with its $k$ nearest neighbours based on Euclidean distance. 
	\item Mutual kNN(m-kNN)\cite{von2007tutorial}. Note the set of kNN is not symmetric. 
		Multual kNN connects those points who are kNN to each other. 
	\item Complete graph\cite{von2007tutorial}. All vertices are connected with 
		each other. This construction is ususally used with Gaussian kernel 
		weighting below. 
\end{itemize}

After the construction of graph, edges can be weighted in several ways:
\begin{itemize}
	\item Unweighted\cite{belkin2003laplacian}. The adjacency matrix is 
		mere 1(connected) or 0(disconnected). 
	\item Gaussian kernel\cite{von2007tutorial}, also 
		called heat kernel\cite{belkin2003laplacian}. Each edge is weighted
		by $A_{ij} = \exp\{-||x_i-x_j||^2/t\}$, where $t$ is a super parameter
		controling the decaying rate of similarity. 
\end{itemize}

Heuristics on selecting $\epsilon, k, t$ are proposed by many authors, 
e.g. ch8 of \cite{von2007tutorial}, but there is no real 
rule of thumb. Since the focus of SET study is on the embedding part, 
most work do not try hard to tweak the construction of similarity graph. 
We propose other possibilities, which may be helpful to target 
different practical problems:
\begin{itemize}
	\item Mahalanobis distance\cite{wiki_md}. The connection condition 
		$||x_i-x_j|| < \epsilon$ is substituted by 
		$\tran{(x_i-x_j)}\Sigma^{-1}(x_i-x_j) < \epsilon^2$. 
		Accodingly, when Gaussian kernel is used, the edge weight
		is given by $A_{ij} = \exp\{-0.5\tran{(x_i-x_j)}\Sigma^{-1}(x_i-x_j)\}$. 
	\item Jaccard's coefficient\cite{wiki_jaccard}. It computes the ratio 
		of the intersection and union two sets. It is useful when 
		the high dimensional input coordinates can be interpreted as 
		sets. 
	\item Cosine similarity\cite{wiki_cos}. It computes the 
		angle between two vectors and is widely used in 
		text mining context. 
\end{itemize}


\subsubsection{Proximity}

Many real problems has proximity as input. Proximity 
can be described by similarity or dissimilarity. 
With pairwise similarity input, we can directly fit the 
data into following SE procedure. A more interesting 
problem is to transform dissimilarity into similarity, 
or at least an equivalent quantity. 
Decomposing the transformed matrix should be able to yield
reasonable embedding(under certain justification). 

Denote data matrix by $X = [x_1, x_2, \ldots, x_N]$. Every 
column $x_j$ corresponds to an $n$ dimensional point. We 
calculate the pairwise squared distance by 
$d^2_{ij} = ||x_i - x_j||^2 = \tran{x_i}x_i + \tran{x_j}x_j - 2\tran{x_i}x_j$. 
Grouping the $N^2$ entries into matrix form, we have:
\begin{equation}
	D^{(2)} = c\tran{\vec{1}} + \vec{1}\tran{c} - 2 \tran{X}X
\end{equation}
where $c$ is a column vector with $\tran{x_i}x_i$ being the entries. 
We'll see later (\rsec{\ref{sec:mds}}) that once a matrix $B$ can be 
written in the form $B = \tran{X}X$, we have standard ways to decompose
it and recover the low dimensional embedding. That is, given 
dissimilarity measures, we can construct the corresponding inner 
product form. A standard approach is double centering: \cite{borg2005modern}
\begin{eqnarray}
	J &=& I - \frac{1}{n} \vec{1} \tran{\vec{1}} \\
	\tran{X}X &=& -\frac{1}{2}JD^{(2)}J
\end{eqnarray}

There are yet two problems left:
\begin{itemize}
	\item First, not all dissimilarities are distance. One reason is 
		that the real world data is with noise. In such case, 
		random noise will be reduced by SET. Another reason, 
		maybe more frequently seen, is data inconsistency. 
		This is quite often seen in computational psychology 
		when people try to rate the degree of dissimilarity of 
		objects\cite{borg2005modern}. Those empirical data 
		may break distance laws, like triangle inequality. 
	\item Second, in order to make double centering work, 
		the low dimensional data are required to zero mean, 
		namely $\sum_i{x_i} = 0$. Actually, we can choose any 
		points as the origin(rather than sample mean), and 
		corresponding forms can be derived. Since our input in this case 
		is pairwise distance($D$ or $D^{(2)}$), their is no 
		explicit definition of origin. Or in another word, 
		the effect of embedding is invariant under translation. 
		We can safely locate the embedded sample mean at the origin. 
\end{itemize}

In total, given dissimilarity matrix $D$, we 
take the element wise square $D^{(2)}$ and then 
pass the double centered version $-\frac{1}{2}JD^{(2)}J$
to next stage. 

\subsubsection{Enhancements}

The above sections discussed the basic formulation of an 
adjacency matrix $A$. They can be fit direcly into SE
procedures. At the same time, people propose some 
enhancement techniques based on certain assumptions. 

Geodesic distance, as is in isomap\cite{tenenbaum2000isomap}, 
computes all pair shortest path(geodesic distance) 
of the adjacency graph first. Then the pairwise geodesic distance 
is treated as normal distance, $D$. The standard MDS
(\rsec{\ref{sec:mds}}) can 
construct an embedding for $D$. For details, please see 
\rsec{\ref{sec:isomap}}. 

Although we have not found other widely used enhancements
in literature, the discussion here do reveal other possibilities. 
For example, what will be the result if we plug in effective 
resistance as distance and fit in later SE stage? The effective 
resistance is closely related with commute time\cite{lau-2012-spectral-ln}. 
If it yield good results in practice, justification will not 
be hard. 

% and conductance

\subsection{Spectral Embeding}

\subsection{Clustering}


\section{Spectral Clustering Justification}
\label{sec:justification}

\subsection{Random Walk}
[von]

\subsection{Normalized Cut}
\label{sec:ncut}
[shi]

\subsection{Normalized Association}
\label{sec:nassoc}
[shi]

\subsection{Ratio Cut}	
[von]

\subsection{Conductance}
[von]

\subsection{Matrix Perturbation}
[andrew ng]

\subsection{Low Rank Approximation}
[matthew brand]

\subsection{Density Estimation View}
[mo chen, 2010]

\subsection{Commute Time}
[jihun ham, kernel]. 
view pseudo inverse of graph Laplacian by commute times on graphs. 

\subsection{Polarization}
[m. brand] unifying view... 

\section{Other Spectral Embedding Technique}
\label{sec:nldr}

\subsection{MDS}
\label{sec:mds}

\subsection{isomap}
\label{sec:isomap}


\subsection{Laplacian Eigenmap}
\label{sec:lemap}

\subsection{Hessian Eigenmap}
\label{sec:hemap}

\subsection{PCA}
\label{sec:pca}

\subsection{LLE}
\label{sec:lle}

\subsection{Kernel PCA}
\label{sec:kpca}

\subsection{Kernel Framework}
\label{sec:kfx}

\subsection{Graph Framework}
\label{gfx}


\section{Conclusion}
\label{sec:conclusion}


%>============================================
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

%<=======Acknowledgements ENd=================

%>============================================
\addcontentsline{toc}{section}{References}
\input{../reference/gen_bib.bbl}
%<=======Bibliography ENd=====================

%>============================================
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

%<=======Appendix ENd=========================

\end{document}
