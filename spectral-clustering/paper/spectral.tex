%Modify the relative path accordingly
\input{common.tex}

\usepackage{algorithm}
\usepackage{algorithmic}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{subfigure}

\fancyhead[LO,LE]{HU, Pili}
\fancyhead[RO,RE]{Spectral Clustering Survey}

%This usually doesn't need modification 
\author{HU, Pili\thanks{hupili [at] ie [dot] cuhk [dot] edu [dot] hk}}

%Modify them accordingly===
\title{Spectral Clustering Survey}
\date{May 14, 2012\thanks{Last compile:\today}}

\begin{document}

\maketitle
%>============================================
\begin{abstract}
	Abstract. Sources can be found in \cite{hu2012-spectral2hop}. 
\end{abstract}
%<=======Abstract ENd=========================

%>============================================
\pagebreak
\setcounter{tocdepth}{2}
\tableofcontents
\pagebreak
%<=======TOC ENd==============================



\section{Introduction}
\label{sec:introduction}

Spectral Clustering(SC) was used in several disciplines long ago. 
For example, computer vision\cite{shi2000normalized}.
Spectral Embeding(SE) was 
also widely discussed in the community\cite{brand2003unifying}. 
Outside spectral community, the machine learning community also 
developed many linear or non-linear Dimensionality Reduction(DR) methods, 
like Principal Component Anslysis (PCA), Kernel PCA (KPCA)\cite{scholkopf1998kpca}, 
Locally Linear Embedding (LLE)\cite{roweis2000lle}, etc. 
Other technique like Multi-Dimensional Scaling(MDS) was successfully 
used in computational psychology for a very long time\cite{borg2005modern}, 
which can be viewed as both "embedding" or "dimensionality reduction".

%, load balancing
%\cite{hendrickson1993multidimensional}, electronics design
%\cite{hadley1992efficient}, etc.

According to our survey, although those methods target at different problems
and are derived from different assumptions, they do share a lot in common. 
The most significant sign is that, the core procedure involves 
eigenvalue decomposition or singular value decomposition, aka "spectral". 
They all involve an intermidiate step of embedding high-dimensional / 
non-Euclidean / non-metric points into a low-dimensional Euclidean space
(although some do not embed explicitly). In this case, we categorize all 
these algorithms as Spectral Embedding Technique(SET). 


\subsection{A Sample Spectral Clustering Algorithm}

%%Following is just an example copied from:
%%http://en.wikibooks.org/wiki/LaTeX/Algorithms_and_Pseudocode
%\begin{algorithm}                      % enter the algorithm environment
%\caption{Calculate $y = x^n$}          % give the algorithm a caption
%\label{alg1}                           % and a label for \ref{} commands later in the document
%\begin{algorithmic}                    % enter the algorithmic environment
%    \REQUIRE $n \geq 0 \vee x \neq 0$
%    \ENSURE $y = x^n$
%\end{algorithmic}
%\end{algorithm}

There are many variations of SC. They all work under certain conditions
and researchers don't have a rule of thumb so far. Before we analyze their
procedure and justification, we present a simple but workable sample 
algorithm(\ralg{\ref{alg:sc_sample}}). 

\begin{algorithm}[htb]
	\caption{Sample Spectral Clustering}
	\label{alg:sc_sample}
	\begin{algorithmic}[1]
		\REQUIRE Data matrix $X = [x_1, x_2, \ldots, x_N]$;  
		Number of Clusters $K$. 
		\ENSURE Clustering $\{C_i\}$: $C_i \in V$ 
			and $\cap_i C_i = \emptyset$
			and $\cup_i C_i = V$. 
		\STATE Form adjacency matrix $A$ within $\epsilon$-ball.
		\STATE Solve $A = U \Lambda \tran{U}$, indexed according 
		the eigenvalue's magnitude. 
		\STATE $Y \leftarrow$ first $K$ columns of U. 
		\STATE Cluster $Y$'s rows by K-means. 
	\end{algorithmic}
\end{algorithm}

In \ralg{\ref{alg:sc_sample}}, the $\epsilon$-ball adjacency graph is 
constructed as follows. First create one vertex for each data point. 
If for two points $i,j$ satisfy $||x_i-x_j|| < \epsilon$, connect 
them with an edge. In this simple demonstration, we consider an unweighted
graph, i.e. all entries of $A$ are 0(disconnected) or 1(connected). 

\begin{figure}
	\centering
	\subfigure[Data Scatter Plot]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_scatter.png}	
		\label{fig:ssc_data}
	}
	\subfigure[Standard K-means]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_kmeans.png}	
		\label{fig:ssc_kmeans}
	}
	\subfigure[Adjacency Graph]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_adj.png}	
		\label{fig:ssc_adj}
	}
	\subfigure[Sample SC Algorithm]{
		\includegraphics[width=0.4\textwidth]{../plot/sc_sample_sc.png}	
		\label{fig:ssc_sc}
	}
	\caption{Demonstration of Sample SC Algorithm}
	\label{fig:ssc_demo}
\end{figure}

\rfig{\ref{fig:ssc_demo}} demonstrates the result of our sample SC algorithm, 
compared with standard K-means algorithm. 
\rfig{\ref{fig:ssc_data}} shows the scatter plot of data. 
It is composed of one radius 1 circle and another radius 2 circle, 
both centered at (1,1). 
\rfig{\ref{fig:ssc_kmeans}} shows the result of standard K-means
working on Euclidean distance. 
\rfig{\ref{fig:ssc_adj}} shows the graph representation, where 
the adjacency graph is formed by taking a $\epsilon$-ball and 
$\epsilon=0.7$ in the example. 
\rfig{\ref{fig:ssc_sc}} shows the output of \ralg{\ref{alg:sc_sample}}. 
It's obvious that standard K-means algorithm can not correctly cluster 
the two circles. This is a known major weakness of K-means(in Euclidean): 
When clusters are not well separated spheres, it has difficulty recovering 
the underlying clusters. Although K-means works for this case 
if we transform the points 
into polar coordinate system(see \cite{hu2012-spectral2hop} for code), 
the solution is not universal. However, in this example, our sample SC 
algorithm can separate the two clusters, probably because the eigenvectors
of adjacency matrix convey adequate information. 

A precaution is that \ralg{\ref{alg:sc_sample}} does not always work even 
in this simple case. Nor have we seen this algorithm from formally published works
(so far), let alone justifications. This algorithm is only to show the flavour 
of spectral clustering and it contains those important steps in other
more sophisticated algorithms. Readers are recommended to learn 
von Luxburg's tutorial\cite{von2007tutorial} before reading the following sections. 
Since that paper is very detailed, we'll present overlapping topics 
concisely. 


\subsection{Spectral Clustering Taxonomy}

When the readers start to survey SC related topics, they will soon find that
there are two streams of study:
\begin{itemize}
	\item \textbf{Embedding Like}. 
		An example is presented in 
		\ralg{\ref{alg:sc_sample}}. One of the core procedure is 
		an embedding of points into lower dimensional Euclidean space. 
		After that, hard cut algorithm like K-means is invoked to 
		get the final result. 		
		This stream has a lot in common with SE and
		DR. In the current article, we focus on this line of research. 
	\item \textbf{Partitioning Like}. 
		One early example is the 
		2-way cut algorithm presented by Shi and Malik in\cite{shi2000normalized}.
		Later Kannan et al. analyzed the framework further\cite{kannan2004clusterings}. 
		The core procedure of this type of algorithm is a 2-way 
		partitioning subroutine. By invoke this subroutine on 
		resultant subgraph repeatedly, we can get final $K$ clusters. 
		When the subroutine can guarantee certain quality, 
		the global resultant clustering quality can be guaranteed
		\cite{kannan2004clusterings}. The attracting aspect of Kannan's framework
		is that, we can plugin any subroutine as long as they have 
		quality gurantee in one cut. For example, the eigen vector of 
		left normalized graph Laplacian can induce good 2-way cut 
		in terms of the Normalized Cut\cite{shi2000normalized}
		(\rsec{\ref{sec:ncut}}). This line of research is more close 
		to Spectral Graph Partitioning(SGP), although those algorithms also get the name 
		of Spectral Clustering. 
\end{itemize}

It's worth to note some work of SGP. In Spielman's\cite{spielman-2009spectral-ln} 
and Lau's\cite{lau-2012-spectral-ln} lecture notes, they both presented a 
graph partitioning algorithm said to result from Lov{\'a}sz\cite{lovasz1990mixing}. 
Chung\cite{chung2007random} made this framework clear: First define a function 
$f(v)$ on all vertices; Then set threshold $T$ to cut vertices into two groups, 
$\{v|f(v) \le T\}$ and $\{v|f(v) > T\}$. The question now becomes to find a 
good heuristic function $f$. Note that the original bi-partition problem is of
complexity $O(2^N)$ and the heuristic based bi-partition problem is of 
$O(N)$(plus the time to get $f$). If we define $f$ as the second eigen vector 
of left normalized graph Laplacian, it is just the algorithm of Shi and
Malik\cite{shi2000normalized}. Besides, there can be multiple $f_i$ and 
the best cut can still be searched in polynomial time
(given $f_i$). For example, in 
\cite{lau-2012-spectral-ln} Lau used random walk probability 
$P_1, P_2, \ldots, P_t$ as $f$. Some recent research of the heuristic function
are Personalized Pagerank\cite{andersen2007detecting}, 
Evolving Sets\cite{andersen2009finding}, etc. 

In the rest of this article, we refer Spectral Clustering to the 
first type, i.e. the embedding like algorithms. 



\subsection{Linear Algebraic Properties}




\section{Spectral Clustering Framework}
\label{sec:framework}



\subsection{Metric Formulation}

\subsection{Spectral Embeding}

\subsection{Clustering}


\section{Spectral Clustering Justification}
\label{sec:justification}

\subsection{Random Walk}
[von]

\subsection{Normalized Cut}
\label{sec:ncut}
[shi]

\subsection{Ratio Cut}
[von]

\subsection{Conductance}
[von]

\subsection{Matrix Perturbation}
[andrew ng]

\subsection{Low Rank Approximation}
[matthew brand]

\subsection{Density Estimation View}
[mo chen, 2010]

\subsection{Commute Time}
[jihun ham, kernel]. 
view pseudo inverse of graph Laplacian by commute times on graphs. 

\subsection{Polarization}
[m. brand] unifying view... 

\section{Other Spectral Embedding Technique}
\label{sec:nldr}

\subsection{MDS}
\label{sec:mds}

\subsection{isomap}
\label{sec:isomap}


\subsection{Laplacian Eigenmap}
\label{sec:lemap}

\subsection{Hessian Eigenmap}
\label{sec:hemap}

\subsection{PCA}
\label{sec:pca}

\subsection{LLE}
\label{sec:lle}

\subsection{Kernel PCA}
\label{sec:kpca}

\subsection{Kernel Framework}
\label{sec:kfx}

\subsection{Graph Framework}
\label{gfx}


\section{Conclusion}
\label{sec:conclusion}


%>============================================
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

%<=======Acknowledgements ENd=================

%>============================================
\addcontentsline{toc}{section}{References}
\input{../reference/gen_bib.bbl}
%<=======Bibliography ENd=====================

%>============================================
\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

%<=======Appendix ENd=========================

\end{document}
