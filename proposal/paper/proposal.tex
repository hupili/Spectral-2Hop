\documentclass[11pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfigure}

\newcommand{\question}{\textbf{---NEED-REVIEW-HERE---}}

\author{HU, Pili}
\title{Spectral Techniques for Community Detection on 2-Hop Topology}

\begin{document}

\maketitle

\begin{abstract}
	In one of our former project\cite{hu2011-cd2hop}, we formulated
	the problem of community detection on 2-hop topology. Given 
	one observer in a social network, and the graph it can reach 
	within two steps, we want to determine for those vertices whether they're
	in the same community with the observer. 
	The previous project extracts features like Common Neighbours, Ademic/Adar
	Score, Pagerank, and Personalized Pagerank, etc. Then several neural 
	networks are trained to combine those features. 
	Interested readers can refer to \cite{hu2011-cd2hop}
	for more information.  
	
	In this project, we augment our research based on the same settings. 
	Several spectral techniques are leveraged to tackle with the 2-hop 
	problem. For instance, spectral clustering may utilize the information 
	hidden in more eigen vectors. Other technique like personalized PageRank 
	can be studied with the budgeted learning context. 
	For data, codes, and other materials, please refer to 
	our open source repository\cite{hu2012-spectral2hop}. 
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}

\subsection{Problem}
Notations:
\begin{table}[htb]
	\centering
	\caption{Notations}
	\label{tbl:notation}
	\begin{tabular}{c|c}
	\hline
	Symbol & Explanation \\
	\hline
	$n$ & node number \\
	\hline
	$m$ & edge number \\
	\hline
	$A$ & adjacency matrix \\
	\hline
	$A_{ij}$ & 1 if node $i$ and node $j$ are directly \\
	& connected, 0 otherwise\\
	\hline
	$N(i)$ & neighbourhood of $i$, namely \\
	& $N(i) = \{j | A_{ij} = 1 \}$ \\
	\hline
	$d(i)$ & degree of node $i$ \\
	& $d(i) = \sum_{j}{A_{ij}}$ \\
	\hline
	$o$ & observer \\
	\hline
	$l_i$ & the real label of node $i$ \\
	\hline
	$L_i$ & the predicted label of node $i$ \\
	\hline
	$N2(i)$ & the 2-hop neighbours we study, defined as \\
	& $\cup_{j \in N(i) \cup \{i\}}{N(j)}$\\
	\hline
	\end{tabular}
\end{table}

Problem settings:
\begin{itemize}
	\item Give observer $o$. 
	\item Give the two hop topology, namely the $N2(o)$ and 
	the associated links. 
	\item Try to predict $L_i$ according to the above information. 
	\item Ground truth $l_i$ is directly crawled from the website. 
\end{itemize}

\subsection{Evaluation}

Our evaluation method distinguishes this work from many 
previous ones. Usual evaluation criterion is defining 
a certain quality functions, like\cite{aggarwal2011social} normalized cut, 
conductance, and modularity. Different research groups 
have their own taste and preference. 

The major problem is, those quality functions can reflect 
the accuracy of algorithms only to a limited extent. 
Let spectral clustering be an example. Luxburg\cite{von2007tutorial}
explains clustering using the notion of finding sets of nodes 
with small conductance in between. In this way, if conductance 
is selectd as the quality function, a good "clustering" is 
a natural consequence of spectral clustering algorithm. 

While those quality functions are simple and intuitive to 
facilitate theoretical study, they can not completely reflect the 
real application performance. In this work, we crawled ground truth
data from one SNS, thus we're able to evaluate the output 
against those crawled label. 

\subsection{Digest of Previous Results}

Overall speaking, we didn't get satisfying result in the previous project. 
The diffusion matrix after combining 9 features with neural network
always show a tradeoff between precision and recall. 

\begin{figure}[htb]
    \label{fig:subfigures}
    \begin{center}
        \subfigure[Common Neighbour]{%
            \label{fig:roc_sub_common}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/common.jpg}
        }%
        \subfigure[Ademic/Adar]{%
           \label{fig:roc_sub_adamic}
           \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/adamic.jpg}
        }
        \subfigure[Jaccard's]{%
            \label{fig:roc_sub_jacc}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/jacc.jpg}
        }\\%

        \subfigure[PR:EV=All 1's]{%
            \label{fig:roc_sub_pr_all1}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_all1.jpg}
        }%
        \subfigure[PR:EV=High 3]{%
           \label{fig:roc_sub_pr_high}
           \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_high.jpg}
        }
        \subfigure[PR:EV=Root]{%
            \label{fig:roc_sub_pr_root}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_root.jpg}
        }\\%
        
        \subfigure[PR:EV=All 1's(+SN)]{%
            \label{fig:roc_sub_pr_all1_super}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_all1_super.jpg}
        }%
        \subfigure[PR:EV=High 3 (+SN)]{%
           \label{fig:roc_sub_pr_high_super}
           \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_high_super.jpg}
        }
        \subfigure[PR:EV=Root(+SN)]{%
            \label{fig:roc_sub_pr_root_super}
            \includegraphics[width=0.3\textwidth]{../roc/pre-o2-fig/pr_root_super.jpg}
        }\\%
%
    \end{center}
    \caption{%
        ROC Curve of 9 Features
     }%
     \label{fig:old_roc_sub_all}
\end{figure}

Fig(\ref{fig:old_roc_sub_all}) shows the single feature evalutation result. 
By varying a threshold, we plot the True Positive Rate v.s. the False Positive 
Rate(label 1, namely "same community"). Those single feature evaluations 
are the useful part of the former project. The observations are:
\begin{itemize}
	\item Three simple features, as claimed by some researchers already 
	performs well in practice. 
	\item Compared with those simple features, more complicated PR
	series exhibits little benefit. 
	\item Among the 6 PR variations, the best performed one is Personalized
	PR with escape vector equal to 3 highest degree nodes from the target 
	community. 
\end{itemize}

\section{Proposed Future Study Line}

\subsection{Evaluation Modification}

From previous study, we find that diffusion matrix is not the most 
convenient way to do evaluation. The diffusion matrix is the final 
result, but highly depends on choice of parameters(may be trained 
weights in neural network). In this work, we want to leverage 
spectral techniques and do a in depth study of them. We find that 
ROC Area(ROCA), namely the area under ROC curve, is more convenient 
to compare. The larger this area, the better an algorithm performs. 
Intuitively speaking, ROCA can be regarded as the expectation of
precision w.r.t threshold, assuming the threshold is chosen at random
uniformly. 

Besides, the previous study finds that the number of leaf nodes is 
so large that they influence our evaluation. i.e. for a normal user 
with 400 buddies, there may be 100,000 nodes within two hop. Among 
the 100,000 nodes, there may be 7,000 nodes belonging to the same 
community with the observer. Under such situation, a naive algorithm 
assigning every node to label 0 can reach an accuracy of above 90\%. 
This outcome is of course not interesting at all. To evaluate the 
current work, we want to limit the scope of training and testing 
sets(full 2-hop topology is still served to algorithms). 

To conclude our evaluation method: 
\begin{itemize}
	\item Use ground truth rather than quality function. 
	\item Each algorithm outputs a "likelihood" value, 
	and we calculate ROCA by varying the threshold of this 
	likelihood value. 
	\item Full 2-hop topology is served to all algorithms, 
	but evaluation is done within a limited scope, like 
	only the level 1 nodes, without degree 1 nodes, etc. 
	Exact choice and justification will be provided in 
	the formal report. 
\end{itemize}

\subsection{Spectral Clustering}

Although we didn't use the term "spectral", we actually applied 
one very useful spectral technique, PageRank. To be short, variations 
of PR are all the principal eigenvector of a certain matrix. 
This value is shown to be informative for finding some local 
sparse cuts\cite{csci5160course}. 

As is known, there are much more eigenvectors that are not used in PR. 
It's no surprise that those successive eigenvectors can help to 
distinguish nodes from different clusters. Luxburg's tutorial\cite{von2007tutorial}
uses a simple example to illustrate this idea. 

Note one thing that although we talk "eigen vectors" in both paragraphs above, 
they corresponds to different matrix. In calculating PR, the matrix 
is obtained from (maybe weighted) adjacency matrix. However, in spectral 
clustering practice, people find the corresponding Laplacian is more useful. 

However, taking as many eigenvectors as possible may not be a good
idea. For one thing, computational cost is higher when dealing with 
more eigenvectors. For another thing, not all eigenvectors are informative. 
Luxburg\cite{von2007tutorial} provides some practical guides on the choice 
of number of eigenvectors. The eigen gap is taken into consideration as a 
general rule. However, bearing in mind that the current work is dealing 
with limited 2-hop topology, there can be proved to be definitely many 
non-informative eigenvectors. Spielman
gives a lemma in his lecture notes\cite{spielman-2009spectral-ln}:
\textbf{if two degree one nodes(say, $i,j$) have a common neighbour, there will 
be a eigen vector taking the $i$-th entry as 1 and $j$-th entry as -1}. 
In our case, large number of level 2 nodes can form this kind of pair. 
The computed eigenvectors may be a linear transformation of such 
standard vectors. The transformed version is also only able to distinguish 
node by node, carrying no cluster information. 

Chanllenges on this line:
\begin{itemize}
	\item How many eigenvector to use? 
	\item What's the number of clusters $k$ used in the 
	meta clustering? 
	\item How to do postprocessing to give final class labels? 
	\item Which form of Laplacian performs is preferred? 
	(Laplacian, Normalized Laplacian, One-side Normalized Laplacian)
	\item Any theoretical guarantee for the above issue? 
	(especially in the context of 2-hop)
\end{itemize}

\subsection{Personalized PR}

The general form of personalized PR is shown in eqn(\ref{eq:ppr}). 
\begin{equation}
	\vec{v} = \alpha A^{\rm T}\vec{v}
	+ (1-\alpha)\frac{\vec{b}}{||\vec{b}||_1}
	\label{eq:ppr}
\end{equation}

If we want to compute the PPR for a set of starting nodes, 
we only need to set all entries corresponding to those nodes 
to 1, and others are left zero. Of course, setting those non-zero
entries to values other than 1 is possible but in the current work 
we only consider 1 for simplicity(meaning no bias on the starting 
set). 

In this work, we'll do a survey on PPR related literature.
We seek for potential theoretical characterization of our problem, 
since in the former work we already find that PPR performs the best
alone. 

\subsection{Budgeted Learning}

In practice, resources are often limited, so we want to do 
budgeted learning. That is, finding the tradeoff between 
cost and performance. 

This research line roots from the observation on PPR:
\begin{enumerate}
	\item PPR can be linearly combined:
\begin{eqnarray}
	\vec{v} &=& \alpha A^{\rm T}\vec{v}
	+ (1-\alpha)\frac{\vec{b}}{||\vec{b}||_1} \\
	&=& \alpha A^{\rm T}\vec{v}
	+ (1-\alpha)\frac{
	\sum_{i \in 
	\text{support($\vec{b}$)}}{e_i}}
	{||\vec{b}||_1} \label{eq:ppr_linear} \\
	&=& \frac{1}{{||\vec{b}||_1}}
	\sum_{i \in \text{support($\vec{b}$)}}{\alpha A^{\rm T}\vec{v}_i}
	+
	\frac{1}{{||\vec{b}||_1}}
	\sum_{i \in \text{support($\vec{b}$)}}{(1-\alpha)e_i}
\end{eqnarray}
where $e_i$ is the unit vector with $i$-th entry equal to 1, and 
$v_i$ is the PPR with only $i$-th vertex in the starting set, namely:
\begin{eqnarray}
	\vec{v_i} &=& \alpha A^{\rm T}\vec{v_i}
	+ (1-\alpha)\vec{e_i}
	\label{eq:ppr_single}
\end{eqnarray}
and we claim that 
\begin{eqnarray}
	\vec{v} = \frac{1}{{||\vec{b}||_1}} \sum_{i \in \text{support($\vec{b}$)}}{\vec{v_i}}
	\label{eq:ppr_decompose}
\end{eqnarray}
This is easy to check by pluging eqn(\ref{eq:ppr_decompose}) and eqn(\ref{eq:ppr_single})
into eqn(\ref{eq:ppr_linear}). 

\item Single node PPR can be computed efficiently using 
Ink Spilling algorithm\cite{csci5160course}. The algorithm 
is theoretically bounded given arbitrary expected precision. 
Computing all PPR w.r.t single node can be done in the initialization 
stage. 
\item In the former study, the success of PPR tells us that 
by serving more label 1 nodes, the performance gets better. One 
extreme case is when all label 1 nodes are served as known nodes 
to the algorithm. In this case, we expect very high ROCA. 
\end{enumerate}

Our application will take the 2-hop topology and some pre-labeled 
nodes as input. The topology is automatically crawled while 
pre-labeling induces certain user side cost. Say, the user 
can label some already known nodes in his community to enhance 
the algorithm performance. However, manual labeling is time consuming, 
so we don't want them to label too many nodes. 

The study of budgeted learning in our problem aims at finding a 
small subset of nodes that help enhance performance. 
The subset can be found in batch mode or in an interactive
manner. 

\subsection{Visualization}

Visualization is not the main target in the current work. 
However, visualization would be a natural by-product 
when we conducting those experiments mentioned above. 

For example, in spectral clustering, we already computed 
the first several eigenvectors. Spielman has shown that
\cite{spielman-2009spectral-ln} plotting the graph using 
the 2nd and 3rd eigen vectors of the Laplacian results 
in very good quality. This 2-D embedding preserves 
original topology the most and try to stretch the vertices 
as out as possible. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/old/sample1W_3.png}
	\caption{An Old Visualization Using NodeXL}
	\label{fig:vis_nodexl}
\end{figure}

Fig(\ref{fig:vis_nodexl}) shows one of the visualization result
using NodeXL in our former project. It's obvious that the general 
graph visualization method there does not suit our requirement 
directly. Besides, NodeXL as a plugin of Excel does not 
works very efficiently. Our typical graph has more than 100K edges. 
In order to make NodeXL work, we sample 10K edges uniformly. 
Taking the physical background into consideration, visualization can 
be improved in the following way:
\begin{itemize}
	\item Treat level one nodes and pure level two nodes separately. 
	Plot the latter one insignificantly and focus them around corresponding
	level one nodes(note that the NodeXL plotting stretched many such nodes
	out, which are not interesting).
	\item Use probably the 2nd and 3rd Laplacian to embed the level one nodes to 
	a 2D plot. Which Laplacian to use is a question. 
	The Laplacian without level two nodes is reasonable. 
	\item Use the spectral clustering result to assign different color 
	to those nodes. 
\end{itemize}

\subsection{Cluster Ensemble}

This is a practical problem but may be not related with spectral techniques directly. 
I put here as a memo. 

Assume we already have a well performed algorithm that takes the data from 
one observer as input and output class label. Our intuition is that, 
by engaging more observers the result has potential to be improved. 
There are two ways to combine information. The first way is to combine raw data. 
That is, we first merge the graphs from two observers and try to 
mine something from the merged graph. This method requires us to 
tune our already extablished algorithm, since it is originally intended 
for only one observer. The second way is to do cluster ensemble. 
By leveraging original algorithm on different observer, we already 
get some (should be) different partitions. The node sets of different 
observers may have a certain intersection. \cite{strehl2003cluster} 
first proposed the cluster ensemble problem and use mutual information 
as the criterion. They formulated an optimization problem to find
the ensembled cluster. 

\section{Challenges}

\begin{itemize}
	\item Preprocessing. Although in our previous project, 
	a lot of preprocessing was done, they're far from enough. 
	With the extension of the this study, other preprocessign may be needed. 
	\item Large computation. Our node number is on the order 
	of 100K. Many direct matrix operations are prohibited.
	In our former project, we implemented the PR algorithm ourselves 
	in C, which essentially speaking is a practical form of 
	sparse matrix-vector product. This program can give result 
	in several seconds on our data set. However, for spectral 
	clustering, which involves eigen value decomposition, 
	the computational cost is more serious. Current plan comes 
	in two folds:
		\begin{enumerate}
			\item Deploy other high performance libraries, like 
			CLPACK. 
			\item Tailor the raw data. Details need to be designed 
			upon the analysis of the data. 
		\end{enumerate}		  
\end{itemize}

\section{Schedule}

\begin{table}[htb]
	\centering
	\caption{Schedule[2012, April - May]}
	\begin{tabular}{|c|c|c|}
	\hline
	Time Period & Content & Expected Output \\
	\hline
	April Week 1-2 & PPR & Survey \\
	April Week 2 & Ink spilling algo. & Implementation \\
	April Week 3 & Budgeted Learning & Sensitivity Simulation \\
	April Week 4 & Spectral Clustering & Implementation\\
	May Week 1 & Sum up & Report \\
	\hline
	\end{tabular}
\end{table}

The current schedule is subject to change according to intermediate outcomes. 
Initial focus is to do survey, implementation, and simulation. 
We expect the successful application of those spectral techniques
in our problem. Theoretical 
analysis will be given if there are considerably good results. 


\input{../reference/gen_bib.bbl}

\end{document}
